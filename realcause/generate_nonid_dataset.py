import numpy as np
from scipy.special import expit
from pathlib import Path

"""
In the observational dataset generated by this code, T (treatment) and Y (outcome)
are confounded by an unobserved confounder U. Since U is not included in
the covariates the usual ignorability assumption (Y(0),Y(1) \bot T | X)
is violated making both ATE and ITE non-identifiable from the observed data.
"""

import numpy as np
from scipy.special import expit


def generate_observational_data(N=1000, seed=42):
    """
    Generates a synthetic observational dataset in which treatment T and outcome Y
    are confounded by an unobserved variable U. This dataset is intended to mimic a
    scenario where the Average Treatment Effect (ATE) and Individual Treatment Effect (ITE)
    are not identifiable from (X, T, Y) alone because U is latent (unobserved).

    -------------------------------------------------------------------------
    Structural Causal Model (SCM) Description
    -------------------------------------------------------------------------
    To save some space and make this more readable, I'm omitting the noise terms.
    U and all X_i have a noise term and their mechanism if the identity.
    When saying U ~ Normal(0, 1), that means U = f_U(U_{exogenous})
    and U_{exogenous} ~ Normal(0, 1) and f_U is the identity mapping.

    TLDR:
        U ~ N(0,1)
        X ~ N(0,1)^d
        Pr(T=1 | X,U) = sigmoid(a_0 + a_1 * X + a_2 * U)
        Y = β_0 + β_1 * T + β_2 * X + β_3 * U + ε where ε ~ N(0, sigma^2)
        final dataset: (X,T,Y)_i, not including U

    We define the following structural equations:

    1) Unobserved Confounder:
       U ~ Normal(0, 1)

    2) Observed Covariates:
       X ~ Normal(0, 1)^d

    3) Treatment Assignment (Binary):
       T ~ Bernoulli( p_T ),   where   p_T = logistic( alpha_0 + alpha_1 * sum(X) + alpha_2 * U )
       - "logistic" means sigmoid function - expit(.)
       - alpha_0, alpha_1, alpha_2 are parameters that define how X and U jointly affect T.
       - In causal notation, T = f_T(X, U, some noise), but here we directly model P(T=1).

    4) Potential Outcomes (Noiseless Mean Functions):
       The "mean outcome" functions mu0 and mu1 correspond to the mean outcome Y for T=0 and T=1, respectively:

         mu0 = beta_0 + beta_2 * sum(X) + beta_3 * U
         mu1 = (beta_0 + beta_1) + beta_2 * sum(X) + beta_3 * U

       - beta_0, beta_1, beta_2, beta_3 are parameters describing the outcome equation.
       - beta_1 is the "structural" difference between mu0 and mu1 (treatment effect),
         but this causal effect is confounded by U.
         e.g.
            Y(0) = mu0 + noise_for_T=0
            Y(1) = mu1 + noise_for_T=1

    5) Observed Outcome (Factual):
       Y (factual) =  Y^obs =  T * ( mu1 + noise_for_T=1 )  +  (1 - T) * ( mu0 + noise_for_T=0 )
       - Each potential outcome has its own noise term (Gaussian with std = sigma).

    6) Counterfactual Outcome:
       Y (counterfactual) =  Y^cf =  T * ( mu0 + noise_for_T=0_cf )  +  (1 - T) * ( mu1 + noise_for_T=1_cf )
       - This is the outcome the individual *would have* had under the opposite treatment.
       - In practice, we never observe Y^cf, but we generate it here for benchmarking.

    7) Average Treatment Effect (ATE):
       ate = E[ mu1 - mu0 ]
       - This is the mean (over all individuals) of the difference between mu1 and mu0.
       - We compute it from the true mu0, mu1 (which includes U).

    -------------------------------------------------------------------------
    Key Points
    -------------------------------------------------------------------------
    - U is NOT returned in the final dataset, so the backdoor path T <-- U --> Y
      cannot be blocked by adjusting for X alone.
    - T and Y are determined in part by U, creating confounding.
    - The "alpha" parameters (alpha_0, alpha_1, alpha_2) parameterize the logistic
      (Bernoulli) structural equation for T.
    - The "beta" parameters (beta_0, beta_1, beta_2, beta_3) parameterize the
      outcome equation for Y, specifying how T, X, and U affect Y.
    - All other mechanisms are the identity function.

    -------------------------------------------------------------------------
    Returns
    -------------------------------------------------------------------------
    A dict with the following keys (shapes in parentheses):
      - 'x'   : Observed covariates (N, d)
      - 't'   : Observed treatment assignments (N,)
      - 'yF'  : Observed (factual) outcome (N,)
      - 'ycf' : Counterfactual outcome (N,)
      - 'mu0' : Potential outcome mean if T=0 (no noise) (N,)
      - 'mu1' : Potential outcome mean if T=1 (no noise) (N,)
      - 'ate' : True average treatment effect (1,)

    By design, 'u' (the unobserved confounder) is not included in the final dict.

    :param N: Number of samples/individuals
    :param seed: Random seed for reproducibility
    :return: A dictionary containing the generated synthetic data
    """
    np.random.seed(seed)

    # ------------------------
    # 1) Dimensions & Parameters
    # ------------------------
    d = 5  # number of observed covariates X_i
    alpha_0 = 0.0
    alpha_1 = 1.0  # coefficient for sum(X) in T's logistic equation
    alpha_2 = 1.0  # coefficient for U in T's logistic equation

    beta_0 = 1.0  # baseline outcome level
    beta_1 = 2.0  # structural effect of T (added to baseline)
    beta_2 = 0.5  # effect of sum(X) on outcome
    beta_3 = 0.5  # effect of U on outcome
    sigma = 0.5  # standard deviation of outcome noise

    # ------------------------
    # 2) Sample Covariates (X) and Confounder (U)
    # ------------------------
    X = np.random.normal(0, 1, size=(N, d))
    U = np.random.normal(0, 1, size=(N,))

    # ------------------------
    # 3) Treatment Assignment T
    # ------------------------
    # T is binary, T ~ Bernoulli( logistic(alpha_0 + alpha_1 * sum(X_i) + alpha_2 * U_i) )
    # Scalar summary for each individual, sum of all covariates:
    sumX = np.sum(X, axis=1)
    logit_T = alpha_0 + alpha_1 * sumX + alpha_2 * U
    p_T = expit(logit_T)  # logistic function to get probability
    T = np.random.binomial(n=1, p=p_T, size=(N,))  # sample N of them, one per datapoint

    # ------------------------
    # 4) Potential Outcome Means (Noiseless)
    # ------------------------
    mu0 = beta_0 + beta_2 * sumX + beta_3 * U  # outcome if T=0
    mu1 = beta_0 + beta_1 + beta_2 * sumX + beta_3 * U  # outcome if T=1

    # ------------------------
    # 5) Observed (Factual) Outcome
    # ------------------------
    # Separate noise for the factual outcome under T=0 or T=1
    noise0 = np.random.normal(0, sigma, size=(N,))
    noise1 = np.random.normal(0, sigma, size=(N,))

    yF = T * (mu1 + noise1) + (1 - T) * (mu0 + noise0)

    # ------------------------
    # 6) Counterfactual Outcome
    # ------------------------
    # Generate separate noise terms for the "opposite" potential outcome.
    noise0_cf = np.random.normal(0, sigma, size=(N,))
    noise1_cf = np.random.normal(0, sigma, size=(N,))

    # If T=1 => CF is mu0; If T=0 => CF is mu1
    y0_cf = mu0 + noise0_cf
    y1_cf = mu1 + noise1_cf
    ycf = T * y0_cf + (1 - T) * y1_cf

    # ------------------------
    # 7) ATE
    # ------------------------
    # ate_value = np.mean(mu1 - mu0) = E[mu1 - mu0]
    # = E[(β_0+β_1+β_2 * sum(X) + β_3 * U)-(β_0+β_2 * sum(X) + β_3 * U)]
    # = β_1 !!!!!!
    print(f"Estimate ATE from dataset: {np.mean(mu1 - mu0)}")
    print(f"True ATE: {beta_1}")
    ate_value = beta_1
    ate = np.array([ate_value])

    # ------------------------
    # 8) Return Dataset
    # ------------------------
    data_dict = {
        "x": X,  # shape: (N, d)
        "t": T,  # shape: (N,)
        "yF": yF,  # shape: (N,)
        "ycf": ycf,  # shape: (N,)
        "mu0": mu0,  # shape: (N,)
        "mu1": mu1,  # shape: (N,)
        "ate": ate,  # shape: (1,)
    }
    return data_dict


def save_dataset(dataset_dict, filename):
    """
    Saves the dictionary of arrays (similarly to the IHDP format)
    into a .npz file with the keys: x, t, yF, ycf, mu0, mu1, ate.

    :param dataset_dict: dict with keys [x, t, yF, ycf, mu0, mu1, ate]
    :param filename: filename for the .npz file
    """
    np.savez(
        filename,
        x=dataset_dict["x"],
        t=dataset_dict["t"],
        yF=dataset_dict["yF"],
        ycf=dataset_dict["ycf"],
        mu0=dataset_dict["mu0"],
        mu1=dataset_dict["mu1"],
        ate=dataset_dict["ate"],
    )


if __name__ == "__main__":
    dataset_directory = Path("datasets")
    dataset_directory.mkdir(exist_ok=True)
    dataset_path = dataset_directory / "non_id_synthetic_data.npz"

    data = generate_observational_data(N=1000, seed=42)
    save_dataset(data, dataset_path)
    print(f"Dataset generated and saved to {dataset_path}")

    # Sanity check on shapes:
    for k, v in data.items():
        print(k, v.shape if hasattr(v, "shape") else type(v))
